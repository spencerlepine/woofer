Organizing Authentication:
 > In frontend with Firebase Auth, working around the context and mocking a user during testing (with Jest)
 > This complicated app state during testing, conditional rendering
 > Firebase stored users, handled all accounts
> BACKEND also needed to verify firebase credentials during API requests

Database Schema:
  > Creating the system to determine RANDOM dog to swipe on:
        - One Zip code per user?
        - How to implement mileage/range
        - Keep track of existing matches to not duplicate
        - Save zip codes under DOG profile, or ZipCode Table/Database?
        - Use SQL, or NoSQL?
        - Seperate database for chats?
       SOLUTION:
       - Saving multiple zip codes for one dog profile
       - Storing dogs under zip code POOL for easy selection
       - Adding modular function to filter out dog profiles before selecting
       - Use NoSQL for document IDs, since no search is needed (already sorted in zip code pools upon addition/deletion)
       - PRO: Using Mongo, no need for ORM, easy to build in the chat feature alongside
       - CON: Potentially MORE space taken up with Mongo
       - PRO: No predetermined table schema, docs are easy to create/delete and update fields

  > End to End system for User to User interaction making matches
        - Making records to Keep track of when UserA swipes YES/NO on UserB
        - Verifying matches to see when UserA is rejecting/accepting UserB match



Problem: Organizing Data and Following Convetions

Explanation: The frontend/backend needed to follow the same API schema. Data being passed to/from the API also needed to match expected values.
There was a need to for strict conventions to follow throughout the code, to avoid hard-to-solve errors from typos.

Solution:
Using CONSTANTS throughout all code. Change the constants in ONE place, and don't worry about breaking code.
Create database schema from CONSTANTS, so it will match code always.
Used CONTSANTS for API routes.
Used references to the database schema to verify INCOMING and OUTCOMING data. Instead of hoping the data was good, ONLY valid data could be entered strictly.
Everything is tightly coupled with the CONSTANTS and SCHEMAS, referring these in each module.
Keeping the code modular allowed me to have a reusable configuration and constants file. This could be passed around to different files and quickly referenced.

ENV Files?:
Context: Need enviroment variables for tokens and config.
Problem: Docker was building with .env in it.
Solution:
 - Ignore .env file, pass in .env file during docker-compose up
 - Organized .env to development and production files for clearer distinction
 - Easier to reuse these files

CI/CD Pipeline Optimization:
Context: using GitHub actions for each task, testing, building, deploying
Problem: SLOW GitHub Actions => Installing packages and setting up enviroments OVER and OVER
Solution:
 - Using artifacts to cache npm modules??
 - Not sure how to optimize GitHub actions yet, just trying to get it working

GitHub actions setup:
Context: Trying to organize testing, building, and deploying ALL in GitHub actions
Problem: Getting very messy, hard to tell what fails
Solution:
 - Make reusable workflows
 - Identify what CI/CD process fails
 - Find error details with context easier


Deploying the Docker Image to EC2
Context: Building on M1 Mac chip and pushing to DockerHub
Problem: building different architectures, could not run on EC2 ubuntu
Solution: qemu emulator on EC2 to run anything, OR build specific architecture before pushing to DockerHub

Docker Image and Enviroment Variables
Context: docker image unable to access enviroment variables
Problem: have file for enviroment variables, it is pushing env files into images to DockerHub
Thinking:
Thinking through it: we want to BUILD/BUNDLE the code, which "brands" the code into bundle files.
 That would mean the env variables get hard coded in UNDER THE HOOD. I cannot BUILD the docker image combining everything, but also leave out the .env.
 Building triggers, Webpack searches for the .env
Solution:
  Store .env in GitHub
  Create the .env file when building
  Reference the .env file in Docker build
  Push image with .env file to DockerHub
  Make production DockerHub image PRIVATE

Broken Deployment ENV variables:
Context: Currently, Server + Client is built into ONE image with docker, which references .env file
Problem:
 - Everytime the server or client updates, it redeploys BOTH
 - If one server fails, the entire docker container needs to restart
 - Server and Client access ENV variables different ways
Solution:
 - Require the image (Dockerfile) to look for .env file on runtime, should be in root directory

CORS Error when running deployed Docker containers
### Problem:
**Context:** Docker image can build and run locally. HOWEVER, the set up is with the server AND client combined, serving static files and acting as the backend API.
**Problem**: This results in`CORS` error on EC2 server.
**Solution:** Use `nginx` to serve files, and run the `Express` server in `parallel`

Managing multiple DockerHub repositories
### Problem #2:
**Context:** Docker compose can build images easily, and referencing backend/frontend ports works. HOWEVER, with a `client`, `server`, and `nginx` folder and Dockerfile, that is 3 different images.
**Problem:** Need to PUSH 3 different docker images to DockerHub, and pull all 3 images again on EC2
**Solution:** Don't go through DockerHub, just clone the GitHub repository and use `docker-compose` on the EC2