Organizing Authentication:
 > In frontend with Firebase Auth, working around the context and mocking a user during testing (with Jest)
 > This complicated app state during testing, conditional rendering
 > Firebase stored users, handled all accounts
> BACKEND also needed to verify firebase credentials during API requests

Database Schema:
  > Creating the system to determine RANDOM dog to swipe on:
        - One Zip code per user?
        - How to implement mileage/range
        - Keep track of existing matches to not duplicate
        - Save zip codes under DOG profile, or ZipCode Table/Database?
        - Use SQL, or NoSQL?
        - Seperate database for chats?
       SOLUTION:
       - Saving multiple zip codes for one dog profile
       - Storing dogs under zip code POOL for easy selection
       - Adding modular function to filter out dog profiles before selecting
       - Use NoSQL for document IDs, since no search is needed (already sorted in zip code pools upon addition/deletion)
       - PRO: Using Mongo, no need for ORM, easy to build in the chat feature alongside
       - CON: Potentially MORE space taken up with Mongo
       - PRO: No predetermined table schema, docs are easy to create/delete and update fields

  > End to End system for User to User interaction making matches
        - Making records to Keep track of when UserA swipes YES/NO on UserB
        - Verifying matches to see when UserA is rejecting/accepting UserB match



Problem: Organizing Data and Following Convetions

Explanation: The frontend/backend needed to follow the same API schema. Data being passed to/from the API also needed to match expected values.
There was a need to for strict conventions to follow throughout the code, to avoid hard-to-solve errors from typos.

Solution:
Using CONSTANTS throughout all code. Change the constants in ONE place, and don't worry about breaking code.
Create database schema from CONSTANTS, so it will match code always.
Used CONTSANTS for API routes.
Used references to the database schema to verify INCOMING and OUTCOMING data. Instead of hoping the data was good, ONLY valid data could be entered strictly.
Everything is tightly coupled with the CONSTANTS and SCHEMAS, referring these in each module.
Keeping the code modular allowed me to have a reusable configuration and constants file. This could be passed around to different files and quickly referenced.

ENV Files?:
Context: Need enviroment variables for tokens and config.
Problem: Docker was building with .env in it.
Solution:
 - Ignore .env file, pass in .env file during docker-compose up
 - Organized .env to development and production files for clearer distinction
 - Easier to reuse these files

CI/CD Pipeline Optimization:
Context: using GitHub actions for each task, testing, building, deploying
Problem: SLOW GitHub Actions => Installing packages and setting up enviroments OVER and OVER
Solution:
 - Using artifacts to cache npm modules??
 - Not sure how to optimize GitHub actions yet, just trying to get it working

GitHub actions setup:
Context: Trying to organize testing, building, and deploying ALL in GitHub actions
Problem: Getting very messy, hard to tell what fails
Solution:
 - Make reusable workflows
 - Identify what CI/CD process fails
 - Find error details with context easier


Deploying the Docker Image to EC2
Context: Building on M1 Mac chip and pushing to DockerHub
Problem: building different architectures, could not run on EC2 ubuntu
Solution: qemu emulator on EC2 to run anything, OR build specific architecture before pushing to DockerHub


Challenge: Deployment Stragety, Continuous Delivery with GitHub Actions to EC2 Instance
Context:
  Docker can be run locally for development. Docker-compose could also be used to run on the EC2, given the entire repository has been cloned.
  Current Deployment Strategy (Git > GitHub > EC2):
	- Development locally (git, GitHub)
	- Push code to Repository (to run GitHub Actions)
	- Trigger the GitHub Actions, run the following steps:
	- Node CI, lint, test, and build server/client.
	- Test `docker-compose up --build` works
	- If everything builds, move on to deployment
	- Connect to EC2 via SSH
	- Note: EC2 instance has node, git, and docker installed
	- `git clone` the repository
	- Install node modules
	- Remove all currently running Docker containers
	- Run `docker-compose up --build` to BUILD the images on the remote machine
	- App is now running, visit EC2 public domain to view production build
Problem:
   EC2 Instance has very limited resources. Cloning the ENTIRE GitHub repository is not practical.
   BUILDING the docker images takes a lot of resources. Must find another way to deploy this.
Solution:
  Use Docker Hub. If Continuous Integration passes, build and publish the Docker images to DockerHub in the GitHub Actions.
  Then, with working images published, SSH into the EC2, and manually download the images to start containers.
  This avoids the BUILDING step on the EC2. Just download the build files and run.
  New Deployment Strategy (Git > Github > DockerHub > EC2):
	- Development locally (git, GitHub)
	- Push code to Repository (to run GitHub Actions)
	- Trigger the GitHub Actions, run the following steps:
	- Node CI, lint, test, and build server/client.
	- Test `docker-compose up --build` works
	- Build and Public Docker CLIENT image to DockerHub
	- Build and Publish Docker SERVER image to DockerHub
	- Connect to EC2 via SSH
	- Note: EC2 instance has only docker installed
	- Remove all currently running Docker containers
 	- Download/Run latest SERVER image from DockerHub
	- Download/Run latest CLIENT image from DockerHub
	- App is now running, visit EC2 public domain to view production build




Docker Image and Enviroment Variables
Context: docker image unable to access enviroment variables
Problem: have file for enviroment variables, it is pushing env files into images to DockerHub
Thinking:
Thinking through it: we want to BUILD/BUNDLE the code, which "brands" the code into bundle files.
 That would mean the env variables get hard coded in UNDER THE HOOD. I cannot BUILD the docker image combining everything, but also leave out the .env.
 Building triggers, Webpack searches for the .env
Solution:
  Store .env in GitHub
  Create the .env file when building
  Reference the .env file in Docker build
  Push image with .env file to DockerHub
  Make production DockerHub image PRIVATE

Broken Deployment ENV variables:
Context: Currently, Server + Client is built into ONE image with docker, which references .env file
Problem:
 - Everytime the server or client updates, it redeploys BOTH
 - If one server fails, the entire docker container needs to restart
 - Server and Client access ENV variables different ways
Solution:
 - Require the image (Dockerfile) to look for .env file on runtime, should be in root directory

CORS Error when running deployed Docker containers
### Problem:
**Context:** Docker image can build and run locally. HOWEVER, the set up is with the server AND client combined, serving static files and acting as the backend API.
**Problem**: This results in`CORS` error on EC2 server.
**Solution:** Use `nginx` to serve files, and run the `Express` server in `parallel`

Managing multiple DockerHub repositories
### Problem #2:
**Context:** Docker compose can build images easily, and referencing backend/frontend ports works. HOWEVER, with a `client`, `server`, and `nginx` folder and Dockerfile, that is 3 different images.
**Problem:** Need to PUSH 3 different docker images to DockerHub, and pull all 3 images again on EC2
**Solution:** Don't go through DockerHub, just clone the GitHub repository and use `docker-compose` on the EC2.




# Docker + Webpack Enviroment Variables Challenge

## Situation
The client cannot bundle the files with webpack ( `npm run build`)  without having all the environment variables already defined in the Node.js runtime. Dockerfile will execute `npm run build`, and it MUST set the ENV variables when building the image.

***Note:** docker can build the server image without ENV vars, and load/reference a `.env` file with `dotenv`  when running a container later.*

## Task
My setup for this project is to read directly from a `.env` file. When building Docker images, this cuases problems, since different libaries handle `process.env` differently.
Everything in this project should reference `process.env` from the Node.js runtime, instead of depending on a literal `.env` file.

The EC2 production server needed to have a .`env` file in the system, but this required me to manually SSH into the machine just create/update the values.


## Action
Used build arguments to pass values to Docker when building images, and set ENV variables with running containers.

- [x] Refactored the Docker file for the client to refrence build ARGS and set ENV varaibles.
- [x] Updated the Docker-compose file to pass build-args when building client image
- [x] Updated the GitHub Action workfows to pass build-args when building the client image
- [x] Updated the GitHub Action workfows to pass env variables when running the client container
- [x] Removed broken refrences to .env file on the EC2 server. Only use ENV variables in the Node.js runtime, not storing a `.env` file on the EC2.

## Review
Webpack is the main problem here. Use `create-react-app` can access `.env` files like the server does.
I was hesitant to refactor the client and remove webpack in case it broke jest.

This was a valuable lesson to explore manaing ENV variables with Docker.
I was able to thoughtfully plan how the project would access ENV vars throughout the build/testing/deployment phases.

Without running into this error, I would have still used a `.env` file manually on the EC2 server.
Now, the GitHub enviroment will store the secrets, and the GitHub Action will set the ENV vars during the deployment.

